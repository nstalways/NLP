{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKkKFYZasOpb"
      },
      "source": [
        "# Reference: [구글 BERT의 정석](http://www.yes24.com/Product/Goods/104491152)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Id66KPVVsOpe"
      },
      "source": [
        "## <span style='color:gold'>01. 트랜스포머의 등장 이유</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjKxRVlosOpf"
      },
      "source": [
        "- 트랜스포머 이전에 RNN, LSTM 네트워크가 존재했다(다음 단어 예측/next word prediction, 기계번역/machine translation, 텍스트 생성/text generation 등의 순차적 태스크에서 널리 사용되었음).\n",
        "- 하지만 이러한 네트워크는 <span style='color:red'>장기 의존성 문제/long-term dependency가 있다.</span>\n",
        ">**[ long-term dependency ]<br>**\n",
        "RNN이 hidden state를 통해 과거의 정보를 저장할 때 문장의 길이가 길어지면 앞의 과거 정보가 마지막 시점까지 전달되지 못하는 현상.<br>\n",
        "-> 모델 학습에 있어 과거의 정보를 활용하지 못하기에 긴 문장이 입력됐을 때 성능이 떨어진다.\n",
        "- 순수하게 어텐션/attention만을 사용한 모델인 트랜스포머는 RNN, LSTM의 한계점을 극복하기 위해 등장했다. (원 논문: [Attention is All You Need](https://arxiv.org/abs/1706.03762))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBT28l_asOpg"
      },
      "source": [
        "## <span style='color:gold'>02. 인코더</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvbkP5SNsOpg"
      },
      "source": [
        "- 트랜스포머의 인코더는 크게 두 가지로 구성되어있다.\n",
        "> **멀티 헤드 어텐션(Multi-Head Attention, MHA)**<br>\n",
        "**피드포워드 네트워크(Feedforward Network)**\n",
        "- MHA를 이해하기 위해선 셀프 어텐션(self attention)을 이해하고 넘어가야 한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kcINcJLsOpg"
      },
      "source": [
        "### 02-01. 셀프 어텐션"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "webj7kSjsOph"
      },
      "source": [
        "**A dog ate the food because it was hugnry.**<br>\n",
        "- 다음과 같은 문장이 있을 때, it은 'food'가 아닌 'dog'를 의미한다.\n",
        "- 사람은 문장 해석을 통해 it이 'dog'를 가리킨다는 것을 알 수 있지만, 모델은 어떻게 알 수 있을까? -> 이 때 self attention을 사용한다.\n",
        "- 모델은 각 단어의 표현을 순차적으로 학습하는데(e.g. A -> dog -> ate -> ...), self attention을 통해 <span style='color:red'>학습하는 과정에서 문장을 구성하는 다른 모든 단어들과의 관계 또한 배우게 된다.</span>\n",
        "- 간단하게 정리하자면 self attention은 **단어와 단어 사이의 관계를 파악**하기 위해 사용하는 것이라고 할 수 있다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hXcg8ixsOph"
      },
      "source": [
        "**self attention의 동작 원리**\n",
        "- self attention을 계산하기 위해선 Q(Query), K(Key), V(Value) matrix가 필요하다.\n",
        "- Q, K, V matrix는 입력 matrix로 부터 생성된다.\n",
        "- 입력 matrix는 (문장 길이 x 임베딩 차원)의 shape을 갖는 matrix이다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ZX6mCQiWsOpi"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2VmAeUksOpj"
      },
      "source": [
        "**[입력 matrix 생성]**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZniUPlKwsOpj",
        "outputId": "9b28b729-f116-46b2-ea06-72f77809c926"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'am', 'good']\n",
            "torch.Size([3, 512])\n"
          ]
        }
      ],
      "source": [
        "input_string = 'I am good'\n",
        "word_list = input_string.split()\n",
        "print(word_list)\n",
        "\n",
        "embedding_dim = 512\n",
        "input_matrix = torch.randn(len(word_list), embedding_dim) # (단어 길이, 임베딩 차원)의 모양을 갖는 입력 행렬 생성\n",
        "print(input_matrix.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5gXjfsNsOpk"
      },
      "source": [
        "**[Q, K, V matrix 생성]**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w_j7faWUsOpk",
        "outputId": "f0928c7b-35ba-424e-a159-5a90ecb527c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "query: torch.Size([3, 64])\n",
            "key: torch.Size([3, 64])\n",
            "value: torch.Size([3, 64])\n"
          ]
        }
      ],
      "source": [
        "qkv_dim = 64\n",
        "weight_Q = nn.Linear(embedding_dim, qkv_dim)\n",
        "weight_K = nn.Linear(embedding_dim, qkv_dim)\n",
        "weight_V = nn.Linear(embedding_dim, qkv_dim)\n",
        "\n",
        "Q = weight_Q(input_matrix)\n",
        "K = weight_K(input_matrix)\n",
        "V = weight_V(input_matrix)\n",
        "\n",
        "print(f'query: {Q.shape}')\n",
        "print(f'key: {K.shape}')\n",
        "print(f'value: {V.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HynLUcRasOpk"
      },
      "source": [
        "**[self attention 계산]**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xBFuartysOpl",
        "outputId": "4069b164-4af9-4337-857b-199c744004d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 3.2006,  1.3753,  0.6912],\n",
            "        [-1.7001,  2.9440, -2.4974],\n",
            "        [-1.1335, -1.5982, -0.7392]], grad_fn=<MmBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# 01. Q와 K.t() 간 내적 연산\n",
        "out = torch.matmul(Q, K.t())\n",
        "print(out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHyscMYSsOpl"
      },
      "source": [
        "- Q와 K.t() 간 내적 연산의 의미는 무엇인가?\n",
        "- Q, K, V 행렬을 구할 때 달라진 것은 임베딩 차원일 뿐, **문장의 길이는 똑같다.** 즉 Q, K, V의 각 행은 'I', 'am', 'good'을 표현하는 벡터들을 의미한다.\n",
        "- Q와 K.t()간의 내적 연산은, 하나의 단어와 전체 단어간의 내적을 구하는 것이라고 생각할 수 있다.\n",
        "- 출력 결과(3x3 matrix)를 보면 이해하기 쉬운데, **<span style='color:red'>(0, 0)은 'I'와 'I'의 내적, (0, 1)은 'I'와 'am'의 내적, (0, 2)는 'I'와 'good'의 내적을 의미한다.</span>**\n",
        "- 내적은 **두 벡터 간 유사성**을 의미한다. 'I'와 'am' 간의 내적은, 두 단어 간 유사성을 구하는 것이라고 설명할 수 있다.\n",
        "- 내적을 기하학적으로 생각해보면, 하나의 벡터를 또 다른 벡터에 투영(projection)시키는 것이다. 벡터의 크기(magnitude)는 특정 벡터를 나타낼 수 있는 속성 중 하나이기에, 투영시킨 벡터의 크기는 곧 두 벡터의 유사도라고 생각할 수 있다.\n",
        "- **결국 out은 단어와 단어 간 연관성을 나타내는 matrix라고 할 수 있다.** (지금은 값이 아무 의미를 나타내지 않지만, 학습 과정을 통해 단어 간 관계를 파악할 수 있게 된다.)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 궁금한 점: 입력 문장 matrix 간의 내적을 구해도 될 텐데, 굳이 Q, K, V matrix를 구하는 이유는 무엇인가?\n",
        "- 내 생각: '학습을 통한 최적화를 가능하게 만들기 위함'이라고 생각한다. 입력 문장 matrix는, 최적화가 불가능한 고정값이다. 허나 Q, K, V matrix는 각각의 weight matrix를 이용해서 생성했기 때문에, 학습 과정을 통한 최적화가 가능하다. Q, K, V matrix가 어떤 방식으로 최적화될 지는 생각하기 어렵지만, loss function을 최적화하는 방향으로 학습될 것은 분명하고 이는 계속해서 정교해진다는 것을 의미한다.\n",
        "- 실제 이유: 아직 안 찾아봄"
      ],
      "metadata": {
        "id": "Ak0Qr5qmuN7j"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uqj9ioP1sOpl",
        "outputId": "cca9ccbe-8151-4bf3-8a34-4f8cd1810a8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.float32\n",
            "8.0\n",
            "tensor([[ 0.4001,  0.1719,  0.0864],\n",
            "        [-0.2125,  0.3680, -0.3122],\n",
            "        [-0.1417, -0.1998, -0.0924]], grad_fn=<DivBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# 02. 학습 안정성을 위해 out을 qkv_dim의 제곱근으로 나누어준다. (특별한 이유는 없고, hyperparameter이다.)\n",
        "# 값을 scaling하면 그만큼 계산되는 gradient도 작아지기 때문에, 학습 시 안정성을 갖출 수 있다.\n",
        "# 미분은 순간적인 변화량이다. -> 값이 큰 경우 변화량 또한 크다. -> 큰 값으로 weight를 업데이트하면 당연히 크게 변한다. -> 학습이 불안정해진다. -> 따라서 scaling이 필요하다.\n",
        "from math import sqrt\n",
        "\n",
        "print(out.dtype)\n",
        "print(sqrt(qkv_dim))\n",
        "out = out / sqrt(qkv_dim)\n",
        "print(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wXZhxthwsOpm",
        "outputId": "1b2ed783-a54d-45f1-fd1a-0a4feedb036f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.3958, 0.3150, 0.2892],\n",
            "        [0.2708, 0.4840, 0.2452],\n",
            "        [0.3340, 0.3151, 0.3509]], grad_fn=<SoftmaxBackward0>)\n",
            "tensor(1., grad_fn=<SumBackward0>)\n",
            "tensor(1.0006, grad_fn=<SumBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# 03. Softmax() 함수를 이용해서 값을 정규화(0~1 사이의 값으로 만드는 것)한다.\n",
        "softmax = nn.Softmax(dim=1)\n",
        "attention_score = softmax(out) # 단어 간 연관성을 나타내는 attention score matrix 연산\n",
        "print(attention_score)\n",
        "\n",
        "print(torch.sum(attention_score[0, :])) # 행 방향(dim=1) 총합이 1인지 확인\n",
        "print(torch.sum(attention_score[:, 0])) # 열 방향(dim=0) 총합이 0이 아닌지 확인"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_qYUHuEsOpm",
        "outputId": "08b20fe6-d5a1-4688-bc00-63b313d157f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 64])\n"
          ]
        }
      ],
      "source": [
        "# 04. attention matrix 연산(주의할 점은, 내적이 아닌 weighted sum!)\n",
        "# weighted sum이기에 이중 for문 말곤 없는 것 같다.\n",
        "\n",
        "row, col = attention_score.shape\n",
        "attention = torch.zeros(V.shape)\n",
        "for i in range(row):\n",
        "    for j in range(col):\n",
        "        attention[i, :] += (attention_score[i][j]*V[i, :]) # score를 하나씩 가져와서, V의 행에 곱한 뒤 attention matrix에 더하기\n",
        "\n",
        "print(attention.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KC4gRlEsOpn"
      },
      "source": [
        "**[self attention의 의미]**\n",
        "\n",
        "- V matrix의 각 행은 각 단어를 표현하는 벡터 값이다.\n",
        "- 거기에 attention score를 곱했다. Attention score는 단어 간 연관성을 나타내는 matrix이다.\n",
        "- 단어 간 연관성과 단어를 곱했기 때문에, 연관성이 높은 단어일 수록 attention matrix에 더 많은 값이 반영될 것이다.\n",
        "- 즉 'I'를 나타내는 attention에는 'I'의 V값이 많이 반영될 것이고, 'am'을 나타내는 attention에는 'am'의 V 값이 많이 반영될 것이다.\n",
        "- 쉽게 설명하자면 **함유량이 다르다고 생각할 수 있다.**\n",
        "- 물리적인 의미를 생각해봤을 때 attention은 **<span style='color:red'>문장의 문맥 정보를 담고 있다고 해석할 수 있겠다.</span>**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 02-02 멀티 헤드 어텐션(Multi-Head Attention, MHA)"
      ],
      "metadata": {
        "id": "Ap1EMELP2ra4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- self attention은 문장의 문맥 정보를 담고 있다(단어 간에 의미가 얼마나 유사한지)\n",
        "- 하지만 주의할 점은, **attention의 학습이 올바른 문맥의 학습을 보장하지 않는다는 점이다.**\n",
        "- 'I am a man.'이라는 문장에서 I는 man과 연관이 있다.\n",
        "- 해당 문장에 대해 attention을 학습시키면, I와 man의 연관성이 올바르게 학습됐을 수도 있고, 아닐 수도 있다.\n",
        "- 한 문장의 경우 쉽다고 생각할 수 있겠지만, 실제 모델 학습에 사용되는 데이터는 엄청나게 많다. 모든 데이터에 대해 모델이 학습하는 과정에서 올바른 문맥을 학습할지 아닐지는 증명이 어렵다.\n",
        "- 그래서 MHA가 등장했다. <span style='color:red'>**잘못된 문맥을 배울 확률을 낮추기 위해서 말이다.**</span> 다르게 말하면 **정확도를 높인다**고 할 수 있다. 일종의 보험이다. \n",
        "- Self attention을 여러 개 둔 것이 Multi-Head Attention이다. Attention 각각을 계산한 다음, concatenate(정해진 축에 따라 연결하는 것)한 뒤 weight matrix와 dot-product 해줌으로써 결과값을 얻는다."
      ],
      "metadata": {
        "id": "jE5G5kVFDua7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Multi-Head Attention 구현\n",
        "import torch\n",
        "\n",
        "# 01. 입력 문장을 벡터로 변환\n",
        "def sentence_to_vector(sentence: str, embedding_dim: int):\n",
        "  sentence = sentence.replace('.', '')  # 단순 불용어 제거. 실제로는 다양한 library의 tokenizer 이용\n",
        "  word_list = sentence.split()          # 공백을 기준으로 분리\n",
        "\n",
        "  return torch.randn((len(word_list), embedding_dim)) # (문장 길이, 임베딩 차원) shape의 가우시안 분포를 따르는 텐서 반환\n",
        "\n",
        "input = 'Transformer is awesome.'\n",
        "embedded_vec = sentence_to_vector(input, 512)\n",
        "print(f'type: {type(embedded_vec)}, shape: {embedded_vec.shape}')"
      ],
      "metadata": {
        "id": "CzkPJbHDIDzB",
        "outputId": "e88c4392-c21b-4b52-b3b7-b792e7eb23f8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "type: <class 'torch.Tensor'>, shape: torch.Size([3, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 02. 원하는 개수 만큼의 Q, K, V matrix 생성\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "torch.manual_seed(0)  # Reproducibility(재현)를 위한 시드 설정. PyTorch의 weight들은 random initialization되기 때문에, 재현을 위해 시드 설정이 필요\n",
        "def qkv_generator(vec: torch.tensor, embedding_dim: int, num: int)-> tuple:\n",
        "  result = []\n",
        "  for _ in range(num):\n",
        "    weight_q = nn.Linear(vec.shape[-1], embedding_dim)\n",
        "    weight_k = nn.Linear(vec.shape[-1], embedding_dim)\n",
        "    weight_v = nn.Linear(vec.shape[-1], embedding_dim)\n",
        "\n",
        "    query = weight_q(vec)\n",
        "    key = weight_k(vec)\n",
        "    value = weight_v(vec)\n",
        "\n",
        "    result.append((query, key, value))\n",
        "  \n",
        "  return tuple(result)\n",
        "\n",
        "qkv_matrices = qkv_generator(embedded_vec, 64, 4)\n",
        "print(f'Total numbers of qkv matrices: {len(qkv_matrices)}')\n",
        "for qkv_matrix in qkv_matrices:\n",
        "  q, k, v = qkv_matrix[0], qkv_matrix[1], qkv_matrix[2]\n",
        "  print(f'query.shape: {q.shape}, query: {q[0, :3]}')\n",
        "  print(f'key.shape: {k.shape}, key: {k[0, :3]}')\n",
        "  print(f'value.shape: {v.shape}, value: {v[0, :3]}')\n",
        "  print()"
      ],
      "metadata": {
        "id": "c80aQJ6JRmrz",
        "outputId": "848cef0a-d785-4206-9415-6c6c2bd39690",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total numbers of qkv matrices: 4\n",
            "query.shape: torch.Size([3, 64]), query: tensor([-0.4001,  0.3124,  0.0872], grad_fn=<SliceBackward0>)\n",
            "key.shape: torch.Size([3, 64]), key: tensor([ 0.5620, -0.0283,  0.3955], grad_fn=<SliceBackward0>)\n",
            "value.shape: torch.Size([3, 64]), value: tensor([0.4760, 0.0479, 0.6244], grad_fn=<SliceBackward0>)\n",
            "\n",
            "query.shape: torch.Size([3, 64]), query: tensor([-0.3928, -0.1612, -0.1454], grad_fn=<SliceBackward0>)\n",
            "key.shape: torch.Size([3, 64]), key: tensor([1.3861, 0.8733, 0.5070], grad_fn=<SliceBackward0>)\n",
            "value.shape: torch.Size([3, 64]), value: tensor([2.0495, 1.5642, 0.1738], grad_fn=<SliceBackward0>)\n",
            "\n",
            "query.shape: torch.Size([3, 64]), query: tensor([-0.3792,  0.7973, -0.2231], grad_fn=<SliceBackward0>)\n",
            "key.shape: torch.Size([3, 64]), key: tensor([-0.4805, -0.3987, -0.4330], grad_fn=<SliceBackward0>)\n",
            "value.shape: torch.Size([3, 64]), value: tensor([ 0.7827, -0.3253,  0.2540], grad_fn=<SliceBackward0>)\n",
            "\n",
            "query.shape: torch.Size([3, 64]), query: tensor([ 0.1879, -0.0828,  1.0953], grad_fn=<SliceBackward0>)\n",
            "key.shape: torch.Size([3, 64]), key: tensor([-1.0027, -0.0695,  0.3784], grad_fn=<SliceBackward0>)\n",
            "value.shape: torch.Size([3, 64]), value: tensor([ 0.1231,  0.3476, -0.4469], grad_fn=<SliceBackward0>)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 03. MHA 구현\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from math import sqrt\n",
        "\n",
        "torch.manual_seed(0)\n",
        "def multi_head_attention(input: tuple)-> torch.tensor:\n",
        "  result = []\n",
        "  softmax = nn.Softmax(dim=1)\n",
        "  for matrix in input:\n",
        "    q, k, v = matrix[0], matrix[1], matrix[2]\n",
        "    attention_score = softmax((torch.matmul(q, k.T)) / sqrt(q.shape[-1]))\n",
        "    \n",
        "    row, col = attention_score.shape\n",
        "    attention_matrix = torch.zeros_like(q)\n",
        "    for i in range(row):\n",
        "      for j in range(col):\n",
        "        attention_matrix[i, :] += (attention_score[i][j] * v[i, :])\n",
        "\n",
        "    result.append(attention_matrix)\n",
        "  \n",
        "  result = torch.cat(tuple(result), dim=-1)  # embedding_dim에 맞추어 concat한다.\n",
        "  weight_mha = nn.Linear(result.shape[-1], input[0][0].shape[-1])\n",
        "  result = weight_mha(result)\n",
        "  print(result.shape)\n",
        "\n",
        "multi_head_attention(qkv_matrices)"
      ],
      "metadata": {
        "id": "YlabprXjUWw6",
        "outputId": "251df5a3-d4ba-41a7-e6f0-65da73878972",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 02-03 위치 인코딩(Positional Encoding)"
      ],
      "metadata": {
        "id": "Lysd2lXKrzIv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 위치 인코딩은 왜 필요한가? 먼저 RNN과 Transformer의 차이를 알아야 한다.\n",
        "> RNN: 모델의 입력이 **단어**이다.<br>\n",
        "Transformer: 모델의 입력이 **문장**이다.\n",
        "\n",
        "- 예를 들어, \"I am good.\" 이라는 문장이 주어졌다고 하자.\n",
        "- 우선 입력 문장에서 불용어(모델 성능에 악영향을 끼칠 수 있는, 무의미한 단어들)들을 제거하고, 단어 단위로 분리한다.\n",
        "- 그 결과로 'I', 'am', 'good'이라는 단어들을 얻을 수 있다.\n",
        "- 컴퓨터는 문자를 알 수 없다. 따라서 각 단어들을 고유한 벡터 값으로 변환한 뒤 모델에 입력한다.\n",
        "- 일련의 과정을 통해 v_I, v_am, v_good 이라는 세 개의 벡터값을 얻었다. 지금부터가 RNN과 Transformer의 차이점이다.\n",
        "---\n",
        "- RNN은 각 벡터 값들을 순차적으로 학습한다.<br>\n",
        "  1. v_I를 입력받고, 학습한다.\n",
        "  1. v_am을 입력받고, 학습한다.\n",
        "  1. v_good을 입력받고 학습한다.\n",
        "- <span style='color:red'>**문장의 의미를 파악하는 것에 있어서, 단어들의 순서 정보는 필수적**이다.</span> 20개의 단어로 구성된 문장이 있다고 할 때, 단어의 순서를 모른다면 해당 문장의 의미를 파악하는 것은 불가능에 가깝다.\n",
        "- RNN은 단어들을 순차적으로 입력받아 학습하기 때문에, 자연스레 순서 정보 또한 학습할 수 있다. 이는 sequential한 모델들의 장점이다.\n",
        "- 단점은 장기 의존성(long term dependency) 문제가 발생한다는 것이다. Sequential 모델들의 공통적인 단점이기도 하다.\n",
        "> 장기 의존성 문제: 문장의 길이가 길어질수록 초기 정보를 잃어버리는 문제\n",
        "- RNN의 학습 방식과 관련이 있는데, 여기서 다루기엔 내용이 너무 길어져서 넘어간다.\n",
        "---\n",
        "- Transformer는 단어 벡터들을 병렬 형태로 합쳐 문장 matrix를 만들고, 이를 학습에 이용한다.\n",
        "- 한 번에 문장 전체를 입력받기 때문에, 장기 의존성 문제에서 훨씬 자유롭다.\n",
        "- 다만 의미 파악에 중요한 단서인 단어들의 순서 정보가 없다. 이를 해결하기 위해 Positional Encoding을 사용하는 것이다.\n",
        "---\n",
        "- 한 가지 의문점: 단어 벡터들을 특정 축에 따라 순서대로 쌓는다면, 이 또한 순서 정보가 아닐까?\n",
        "- 내 생각: 학습 과정에서 순서 정보를 찾아낼 수도 있겠지만, 명시적으로 순서 정보를 주는 것이 성능 면에서 훨씬 좋을 것 같긴 하다.\n"
      ],
      "metadata": {
        "id": "f2BvoQV0r698"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 그렇다면 어떻게 위치 정보를 더해줄 수 있을까?\n",
        "- 정현파(Sinusoidal) 함수를 이용한다.\n",
        "> Sinusoidal function: 주기 함수. 사인(sine) 함수를 지칭하는 말이긴 하지만, 다른 주기함수들을 포함하는 말로 사용하기도 한다.\n",
        "- 행렬 요소들의 위치에 따라 값이 달라지는데, 수식은 다음과 같다.\n",
        "- pos는 문장 속 단어의 위치(row_index)이고, i는 해당 element의 위치를 나타내기 위해 2i 혹은 2i+1에 들어가는 정수값이다.\n",
        "> $$ P(pos, 2i) = sin(\\frac{pos}{10000^{2i/d_{model}}}) $$\n",
        "$$ P(pos, 2i+1) = cos(\\frac{pos}{10000^{2i/d_{model}}}) $$\n",
        "- 잠시 쉬겠습니다.\n"
      ],
      "metadata": {
        "id": "8IctRij7zCPG"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVPFep_BsOpn"
      },
      "source": [
        "## Transformer 최종 구현"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "id": "vuKqVy2lsOpn",
        "outputId": "aad158f6-8362-49fd-bd19-8c8338f554be"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-11-163d1d1219ec>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    class decoder(nn.Module):\u001b[0m\n\u001b[0m                             ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
          ]
        }
      ],
      "source": [
        "class encoder(nn.Module):\n",
        "    def __init__():\n",
        "        super().__init__()\n",
        "    \n",
        "    def forward(self, x):\n",
        "    \n",
        "class decoder(nn.Module):\n",
        "    def __init__():\n",
        "        super().__init__()\n",
        "        \n",
        "    def forward(self, x):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BuUUGlxbsOpo"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__():\n",
        "        super().__init__()\n",
        "        self.encoder = \n",
        "        self.decoder = \n",
        "        \n",
        "    def forward(self, x):\n",
        "        "
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "nlp",
      "language": "python",
      "name": "nlp"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}