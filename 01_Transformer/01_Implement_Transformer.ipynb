{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference: [구글 BERT의 정석](http://www.yes24.com/Product/Goods/104491152)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color:gold'>01. 트랜스포머의 등장 이유</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 트랜스포머 이전에 RNN, LSTM 네트워크가 존재했다(다음 단어 예측/next word prediction, 기계번역/machine translation, 텍스트 생성/text generation 등의 순차적 태스크에서 널리 사용되었음).\n",
    "- 하지만 이러한 네트워크는 <span style='color:red'>장기 의존성 문제/long-term dependency가 있다.</span>\n",
    ">**[ long-term dependency ]<br>**\n",
    "RNN이 hidden state를 통해 과거의 정보를 저장할 때 문장의 길이가 길어지면 앞의 과거 정보가 마지막 시점까지 전달되지 못하는 현상.<br>\n",
    "-> 모델 학습에 있어 과거의 정보를 활용하지 못하기에 긴 문장이 입력됐을 때 성능이 떨어진다.\n",
    "- 순수하게 어텐션/attention만을 사용한 모델인 트랜스포머는 RNN, LSTM의 한계점을 극복하기 위해 등장했다. (원 논문: [Attention is All You Need](https://arxiv.org/abs/1706.03762))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color:gold'>02. 인코더</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 트랜스포머의 인코더는 크게 두 가지로 구성되어있다.\n",
    "> **멀티 헤드 어텐션(Multi-Head Attention, MHA)**<br>\n",
    "**피드포워드 네트워크(Feedforward Network)**\n",
    "- MHA를 이해하기 위해선 셀프 어텐션(self attention)을 이해하고 넘어가야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 02-01. 셀프 어텐션"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A dog ate the food because it was hugnry.**<br>\n",
    "- 다음과 같은 문장이 있을 때, it은 'food'가 아닌 'dog'를 의미한다.\n",
    "- 사람은 문장 해석을 통해 it이 'dog'를 가리킨다는 것을 알 수 있지만, 모델은 어떻게 알 수 있을까? -> 이 때 self attention을 사용한다.\n",
    "- 모델은 각 단어의 표현을 순차적으로 학습하는데(e.g. A -> dog -> ate -> ...), self attention을 통해 <span style='color:red'>학습하는 과정에서 문장을 구성하는 다른 모든 단어들과의 관계 또한 배우게 된다.</span>\n",
    "- 간단하게 정리하자면 self attention은 **단어와 단어 사이의 관계를 파악**하기 위해 사용하는 것이라고 할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**self attention의 동작 원리**\n",
    "- self attention을 계산하기 위해선 Q(Query), K(Key), V(Value) matrix가 필요하다.\n",
    "- Q, K, V matrix는 입력 matrix로 부터 생성된다.\n",
    "- 입력 matrix는 (문장 길이 x 임베딩 차원)의 shape을 갖는 matrix이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[입력 matrix 생성]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'good']\n",
      "torch.Size([3, 512])\n"
     ]
    }
   ],
   "source": [
    "input_string = 'I am good'\n",
    "word_list = input_string.split()\n",
    "print(word_list)\n",
    "\n",
    "embedding_dim = 512\n",
    "input_matrix = torch.randn(len(word_list), embedding_dim) # (단어 길이, 임베딩 차원)의 모양을 갖는 입력 행렬 생성\n",
    "print(input_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Q, K, V matrix 생성]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: torch.Size([3, 64])\n",
      "key: torch.Size([3, 64])\n",
      "value: torch.Size([3, 64])\n"
     ]
    }
   ],
   "source": [
    "qkv_dim = 64\n",
    "weight_Q = nn.Linear(embedding_dim, qkv_dim)\n",
    "weight_K = nn.Linear(embedding_dim, qkv_dim)\n",
    "weight_V = nn.Linear(embedding_dim, qkv_dim)\n",
    "\n",
    "Q = weight_Q(input_matrix)\n",
    "K = weight_K(input_matrix)\n",
    "V = weight_V(input_matrix)\n",
    "\n",
    "print(f'query: {Q.shape}')\n",
    "print(f'key: {K.shape}')\n",
    "print(f'value: {V.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[self attention 계산]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2760,  1.6690,  0.1064],\n",
      "        [-2.9503,  2.3170,  0.0193],\n",
      "        [-2.7349,  1.1041,  1.5544]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 01. Q와 K.t() 간 내적 연산\n",
    "out = torch.matmul(Q, K.t())\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q와 K.t() 간 내적 연산의 의미는 무엇인가?\n",
    "- Q, K, V 행렬을 구할 때 달라진 것은 임베딩 차원일 뿐, **문장의 길이는 똑같다.** 즉 Q, K, V의 각 행은 'I', 'am', 'good'을 표현하는 벡터들을 의미한다.\n",
    "- Q와 K.t()간의 내적 연산은, 하나의 단어와 전체 단어간의 내적을 구하는 것이라고 생각할 수 있다.\n",
    "- 출력 결과(3x3 matrix)를 보면 이해하기 쉬운데, **<span style='color:red'>(0, 0)은 'I'와 'I'의 내적, (0, 1)은 'I'와 'am'의 내적, (0, 2)는 'I'와 'good'의 내적을 의미한다.</span>**\n",
    "- **결국 out은 단어와 단어 간 연관성을 나타내는 matrix라고 할 수 있다.** (지금은 값이 아무 의미를 나타내지 않지만, 학습 과정을 통해 단어 간 관계를 파악할 수 있게 된다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "8.0\n",
      "tensor([[ 0.0345,  0.2086,  0.0133],\n",
      "        [-0.3688,  0.2896,  0.0024],\n",
      "        [-0.3419,  0.1380,  0.1943]], grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 02. 학습 안정성을 위해 out을 qkv_dim의 제곱근으로 나누어준다.\n",
    "# 값을 scaling하면 그만큼 계산되는 gradient도 작아지기 때문에, 학습 시 안정성을 갖출 수 있다.\n",
    "from math import sqrt\n",
    "\n",
    "print(out.dtype)\n",
    "print(sqrt(qkv_dim))\n",
    "out = out / sqrt(qkv_dim)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3155, 0.3756, 0.3089],\n",
      "        [0.2282, 0.4409, 0.3308],\n",
      "        [0.2312, 0.3736, 0.3952]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor(1., grad_fn=<SumBackward0>)\n",
      "tensor(0.7750, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 03. Softmax() 함수를 이용해서 값을 정규화(0~1 사이의 값으로 만드는 것)한다.\n",
    "softmax = nn.Softmax(dim=1)\n",
    "attention_score = softmax(out) # 단어 간 연관성을 나타내는 attention score matrix 연산\n",
    "print(attention_score)\n",
    "\n",
    "print(torch.sum(attention_score[0, :])) # 행 방향(dim=1) 총합이 1인지 확인\n",
    "print(torch.sum(attention_score[:, 0])) # 열 방향(dim=0) 총합이 0이 아닌지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 64])\n"
     ]
    }
   ],
   "source": [
    "# 04. attention matrix 연산(주의할 점은, 내적이 아닌 weighted sum!)\n",
    "# weighted sum이기에 이중 for문 말곤 없는 것 같다.\n",
    "\n",
    "row, col = attention_score.shape\n",
    "attention = torch.zeros(V.shape)\n",
    "for i in range(row):\n",
    "    for j in range(col):\n",
    "        attention[i, :] += (attention_score[i][j]*V[i, :]) # score를 하나씩 가져와서, V의 행에 곱한 뒤 attention matrix에 더하기\n",
    "    \n",
    "print(attention.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[self attention의 의미]**\n",
    "\n",
    "- V matrix의 각 행은 각 단어를 표현하는 벡터 값이다.\n",
    "- 거기에 attention score를 곱했다. Attention score는 단어 간 연관성을 나타내는 matrix이다.\n",
    "- 단어 간 연관성과 단어를 곱했기 때문에, 연관성이 높은 단어일 수록 attention matrix에 더 많은 값이 반영될 것이다.\n",
    "- 즉 'I'를 나타내는 attention에는 'I'의 V값이 많이 반영될 것이고, 'am'을 나타내는 attention에는 'am'의 V 값이 많이 반영될 것이다.\n",
    "- 쉽게 설명하자면 **함유량이 다르다고 생각할 수 있다.**\n",
    "- 물리적인 의미를 생각해봤을 때 attention은 **<span style='color:red'>문장의 문맥 정보를 담고 있다고 해석할 수 있겠다.</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer 최종 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class encoder(nn.Module):\n",
    "    def __init__():\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "    \n",
    "class decoder(nn.Module):\n",
    "    def __init__():\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, x):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__():\n",
    "        super().__init__()\n",
    "        self.encoder = \n",
    "        self.decoder = \n",
    "        \n",
    "    def forward(self, x):\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
