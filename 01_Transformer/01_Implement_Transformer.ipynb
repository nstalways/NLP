{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKkKFYZasOpb"
      },
      "source": [
        "# Reference: [구글 BERT의 정석](http://www.yes24.com/Product/Goods/104491152)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Id66KPVVsOpe"
      },
      "source": [
        "## <span style='color:gold'>01. 트랜스포머의 등장 이유</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjKxRVlosOpf"
      },
      "source": [
        "- 트랜스포머 이전에 RNN, LSTM 네트워크가 존재했다(다음 단어 예측/next word prediction, 기계번역/machine translation, 텍스트 생성/text generation 등의 순차적 태스크에서 널리 사용되었음).\n",
        "- 하지만 이러한 네트워크는 <span style='color:red'>장기 의존성 문제/long-term dependency가 있다.</span>\n",
        ">**[ long-term dependency ]<br>**\n",
        "RNN이 hidden state를 통해 과거의 정보를 저장할 때 문장의 길이가 길어지면 앞의 과거 정보가 마지막 시점까지 전달되지 못하는 현상.<br>\n",
        "-> 모델 학습에 있어 과거의 정보를 활용하지 못하기에 긴 문장이 입력됐을 때 성능이 떨어진다.\n",
        "- 순수하게 어텐션/attention만을 사용한 모델인 트랜스포머는 RNN, LSTM의 한계점을 극복하기 위해 등장했다. (원 논문: [Attention is All You Need](https://arxiv.org/abs/1706.03762))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBT28l_asOpg"
      },
      "source": [
        "## <span style='color:gold'>02. 인코더</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvbkP5SNsOpg"
      },
      "source": [
        "- 트랜스포머의 인코더는 크게 두 가지로 구성되어있다.\n",
        "> **멀티 헤드 어텐션(Multi-Head Attention, MHA)**<br>\n",
        "**피드포워드 네트워크(Feedforward Network)**\n",
        "- MHA를 이해하기 위해선 셀프 어텐션(self attention)을 이해하고 넘어가야 한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kcINcJLsOpg"
      },
      "source": [
        "### 02-01. 셀프 어텐션"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "webj7kSjsOph"
      },
      "source": [
        "**A dog ate the food because it was hugnry.**<br>\n",
        "- 다음과 같은 문장이 있을 때, it은 'food'가 아닌 'dog'를 의미한다.\n",
        "- 사람은 문장 해석을 통해 it이 'dog'를 가리킨다는 것을 알 수 있지만, 모델은 어떻게 알 수 있을까? -> 이 때 self attention을 사용한다.\n",
        "- 모델은 각 단어의 표현을 순차적으로 학습하는데(e.g. A -> dog -> ate -> ...), self attention을 통해 <span style='color:red'>학습하는 과정에서 문장을 구성하는 다른 모든 단어들과의 관계 또한 배우게 된다.</span>\n",
        "- 간단하게 정리하자면 self attention은 **단어와 단어 사이의 관계를 파악**하기 위해 사용하는 것이라고 할 수 있다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hXcg8ixsOph"
      },
      "source": [
        "**self attention의 동작 원리**\n",
        "- self attention을 계산하기 위해선 Q(Query), K(Key), V(Value) matrix가 필요하다.\n",
        "- Q, K, V matrix는 입력 matrix로 부터 생성된다.\n",
        "- 입력 matrix는 (문장 길이 x 임베딩 차원)의 shape을 갖는 matrix이다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ZX6mCQiWsOpi"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2VmAeUksOpj"
      },
      "source": [
        "**[입력 matrix 생성]**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZniUPlKwsOpj",
        "outputId": "6a8f34d7-27bd-4949-f038-e5dd97ead4a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'am', 'good']\n",
            "torch.Size([3, 512])\n"
          ]
        }
      ],
      "source": [
        "input_string = 'I am good'\n",
        "word_list = input_string.split()\n",
        "print(word_list)\n",
        "\n",
        "embedding_dim = 512\n",
        "input_matrix = torch.randn(len(word_list), embedding_dim) # (단어 길이, 임베딩 차원)의 모양을 갖는 입력 행렬 생성\n",
        "print(input_matrix.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5gXjfsNsOpk"
      },
      "source": [
        "**[Q, K, V matrix 생성]**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w_j7faWUsOpk",
        "outputId": "540dbe67-0a76-4e40-f7a3-2a9846a12537"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "query: torch.Size([3, 64])\n",
            "key: torch.Size([3, 64])\n",
            "value: torch.Size([3, 64])\n"
          ]
        }
      ],
      "source": [
        "qkv_dim = 64\n",
        "weight_Q = nn.Linear(embedding_dim, qkv_dim)\n",
        "weight_K = nn.Linear(embedding_dim, qkv_dim)\n",
        "weight_V = nn.Linear(embedding_dim, qkv_dim)\n",
        "\n",
        "Q = weight_Q(input_matrix)\n",
        "K = weight_K(input_matrix)\n",
        "V = weight_V(input_matrix)\n",
        "\n",
        "print(f'query: {Q.shape}')\n",
        "print(f'key: {K.shape}')\n",
        "print(f'value: {V.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HynLUcRasOpk"
      },
      "source": [
        "**[self attention 계산]**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xBFuartysOpl",
        "outputId": "4c566fe3-5db4-4d0d-baec-179ae1ee87cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.3030,  1.7906,  0.0725],\n",
            "        [-1.5858,  2.5118,  1.6223],\n",
            "        [-7.3314,  1.8375, -1.9029]], grad_fn=<MmBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# 01. Q와 K.t() 간 내적 연산\n",
        "out = torch.matmul(Q, K.t())\n",
        "print(out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHyscMYSsOpl"
      },
      "source": [
        "- Q와 K.t() 간 내적 연산의 의미는 무엇인가?\n",
        "- Q, K, V 행렬을 구할 때 달라진 것은 임베딩 차원일 뿐, **문장의 길이는 똑같다.** 즉 Q, K, V의 각 행은 'I', 'am', 'good'을 표현하는 벡터들을 의미한다.\n",
        "- Q와 K.t()간의 내적 연산은, 하나의 단어와 전체 단어간의 내적을 구하는 것이라고 생각할 수 있다.\n",
        "- 출력 결과(3x3 matrix)를 보면 이해하기 쉬운데, **<span style='color:red'>(0, 0)은 'I'와 'I'의 내적, (0, 1)은 'I'와 'am'의 내적, (0, 2)는 'I'와 'good'의 내적을 의미한다.</span>**\n",
        "- 내적은 **두 벡터 간 유사성**을 의미한다. 'I'와 'am' 간의 내적은, 두 단어 간 유사성을 구하는 것이라고 설명할 수 있다.\n",
        "- 내적을 기하학적으로 생각해보면, 하나의 벡터를 또 다른 벡터에 투영(projection)시키는 것이다. 벡터의 크기(magnitude)는 특정 벡터를 나타낼 수 있는 속성 중 하나이기에, 투영시킨 벡터의 크기는 곧 두 벡터의 유사도라고 생각할 수 있다.\n",
        "- **결국 out은 단어와 단어 간 연관성을 나타내는 matrix라고 할 수 있다.** (지금은 값이 아무 의미를 나타내지 않지만, 학습 과정을 통해 단어 간 관계를 파악할 수 있게 된다.)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 궁금한 점: 입력 문장 matrix 간의 내적을 구해도 될 텐데, 굳이 Q, K, V matrix를 구하는 이유는 무엇인가?\n",
        "- 내 생각: '학습을 통한 최적화를 가능하게 만들기 위함'이라고 생각한다. 입력 문장 matrix는, 최적화가 불가능한 고정값이다. 허나 Q, K, V matrix는 각각의 weight matrix를 이용해서 생성했기 때문에, 학습 과정을 통한 최적화가 가능하다. Q, K, V matrix가 어떤 방식으로 최적화될 지는 생각하기 어렵지만, loss function을 최적화하는 방향으로 학습될 것은 분명하고 이는 계속해서 정교해진다는 것을 의미한다.\n",
        "- 실제 이유: 아직 안 찾아봄"
      ],
      "metadata": {
        "id": "Ak0Qr5qmuN7j"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uqj9ioP1sOpl",
        "outputId": "eb8fa9e5-77e6-4dfa-9e31-1ecf09183075"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.float32\n",
            "8.0\n",
            "tensor([[ 0.0379,  0.2238,  0.0091],\n",
            "        [-0.1982,  0.3140,  0.2028],\n",
            "        [-0.9164,  0.2297, -0.2379]], grad_fn=<DivBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# 02. 학습 안정성을 위해 out을 qkv_dim의 제곱근으로 나누어준다. (특별한 이유는 없고, hyperparameter이다.)\n",
        "# 값을 scaling하면 그만큼 계산되는 gradient도 작아지기 때문에, 학습 시 안정성을 갖출 수 있다.\n",
        "# 미분은 순간적인 변화량이다. -> 값이 큰 경우 변화량 또한 크다. -> 큰 값으로 weight를 업데이트하면 당연히 크게 변한다. -> 학습이 불안정해진다. -> 따라서 scaling이 필요하다.\n",
        "from math import sqrt\n",
        "\n",
        "print(out.dtype)\n",
        "print(sqrt(qkv_dim))\n",
        "out = out / sqrt(qkv_dim)\n",
        "print(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wXZhxthwsOpm",
        "outputId": "2afadbdd-138c-4fe7-ba17-37f29612833a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.3149, 0.3792, 0.3059],\n",
            "        [0.2403, 0.4010, 0.3588],\n",
            "        [0.1635, 0.5143, 0.3222]], grad_fn=<SoftmaxBackward0>)\n",
            "tensor(1., grad_fn=<SumBackward0>)\n",
            "tensor(0.7186, grad_fn=<SumBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# 03. Softmax() 함수를 이용해서 값을 정규화(0~1 사이의 값으로 만드는 것)한다.\n",
        "softmax = nn.Softmax(dim=1)\n",
        "attention_score = softmax(out) # 단어 간 연관성을 나타내는 attention score matrix 연산\n",
        "print(attention_score)\n",
        "\n",
        "print(torch.sum(attention_score[0, :])) # 행 방향(dim=1) 총합이 1인지 확인\n",
        "print(torch.sum(attention_score[:, 0])) # 열 방향(dim=0) 총합이 0이 아닌지 확인"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_qYUHuEsOpm",
        "outputId": "92837291-8df2-4ba7-ab14-3f14cd0b5750"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 64])\n"
          ]
        }
      ],
      "source": [
        "# 04. attention matrix 연산(주의할 점은, 내적이 아닌 weighted sum!)\n",
        "# weighted sum이기에 이중 for문 말곤 없는 것 같다.\n",
        "\n",
        "row, col = attention_score.shape\n",
        "attention = torch.zeros(V.shape)\n",
        "for i in range(row):\n",
        "    for j in range(col):\n",
        "        attention[i, :] += (attention_score[i][j]*V[i, :]) # score를 하나씩 가져와서, V의 행에 곱한 뒤 attention matrix에 더하기\n",
        "\n",
        "print(attention.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KC4gRlEsOpn"
      },
      "source": [
        "**[self attention의 의미]**\n",
        "\n",
        "- V matrix의 각 행은 각 단어를 표현하는 벡터 값이다.\n",
        "- 거기에 attention score를 곱했다. Attention score는 단어 간 연관성을 나타내는 matrix이다.\n",
        "- 단어 간 연관성과 단어를 곱했기 때문에, 연관성이 높은 단어일 수록 attention matrix에 더 많은 값이 반영될 것이다.\n",
        "- 즉 'I'를 나타내는 attention에는 'I'의 V값이 많이 반영될 것이고, 'am'을 나타내는 attention에는 'am'의 V 값이 많이 반영될 것이다.\n",
        "- 쉽게 설명하자면 **함유량이 다르다고 생각할 수 있다.**\n",
        "- 물리적인 의미를 생각해봤을 때 attention은 **<span style='color:red'>문장의 문맥 정보를 담고 있다고 해석할 수 있겠다.</span>**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 02-02 멀티 헤드 어텐션(Multi-Head Attention, MHA)"
      ],
      "metadata": {
        "id": "Ap1EMELP2ra4"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVPFep_BsOpn"
      },
      "source": [
        "## Transformer 최종 구현"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "id": "vuKqVy2lsOpn",
        "outputId": "87c5a3a9-183e-4080-a927-1a6a69752f1b"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-9-163d1d1219ec>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    class decoder(nn.Module):\u001b[0m\n\u001b[0m                             ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
          ]
        }
      ],
      "source": [
        "class encoder(nn.Module):\n",
        "    def __init__():\n",
        "        super().__init__()\n",
        "    \n",
        "    def forward(self, x):\n",
        "    \n",
        "class decoder(nn.Module):\n",
        "    def __init__():\n",
        "        super().__init__()\n",
        "        \n",
        "    def forward(self, x):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BuUUGlxbsOpo"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__():\n",
        "        super().__init__()\n",
        "        self.encoder = \n",
        "        self.decoder = \n",
        "        \n",
        "    def forward(self, x):\n",
        "        "
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "nlp",
      "language": "python",
      "name": "nlp"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}